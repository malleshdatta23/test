from airflow.models import Variable
import airflow
from airflow import DAG
from airflow.utils.task_group import TaskGroup
import sys
sys.path.append('/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/')
from datetime import datetime, timedelta
from airflow.utils.db import provide_session
from airflow.operators.dummy import DummyOperator
import os
from pathlib import Path
import json
import re
from collections import defaultdict
from airflow.settings import Session
from airflow.decorators import task
from MDR_Billing.wkfl_NORM_100_500_O_FIN_DAILY_DISC.taskGroups.wklt_NORM_100_500_O_FIN_DAILY_DISC.s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC.m_100_CTL_ETL_FEED_PROCESS_START import *
from airflow.providers.ssh.operators.ssh import SSHOperator
from airflow.providers.ssh.hooks.ssh import SSHHook
from MDR_Billing.wkfl_NORM_100_500_O_FIN_DAILY_DISC.taskGroups.wklt_NORM_100_500_O_FIN_DAILY_DISC.s_m_NORM_300_T_FIN_DAILY_DISC.m_NORM_300_T_FIN_DAILY_DISC import *
from MDR_Billing.wkfl_NORM_100_500_O_FIN_DAILY_DISC.taskGroups.wklt_NORM_100_500_O_FIN_DAILY_DISC.s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT.m_NORM_320_O_FIN_DAILY_DISC_OUTPUT import *
from MDR_Billing.wkfl_NORM_100_500_O_FIN_DAILY_DISC.taskGroups.wklt_NORM_100_500_O_FIN_DAILY_DISC.s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC.m_500_CTL_ETL_FEED_PROCESS_END import *
from operators.dm_status import DMStatusOperator

def get_Status(dag_run, id):
    try:
        status = dag_run.get_task_instance(id).state
        if status == "success":
            return "succeeded"
        if status == "failed":
            return "failed"
        return None
    except Exception:
        return "disabled"

def get_PrevTaskStatus(dag_run, id):
    id = list(dag_run.dag.get_task(id).upstream_task_ids)[0]
    return get_Status(dag_run, id)

def parse_prm_file(paramFilePath):

    abs_path = Variable.get('PARAMFILE_ABS_PATH')
    ssh_id = Variable.get('SSH_CONN_ID')
    ssh_hook = SSHHook(ssh_conn_id=ssh_id)
    ssh_client = ssh_hook.get_conn()
    sftp_client = ssh_client.open_sftp()
    
    remote_expr_path = abs_path + paramFilePath
    stdin, stdout, stderr = ssh_client.exec_command(f'echo {remote_expr_path}')
    resolved_path = stdout.read().decode().strip()
    error_output = stderr.read().decode().strip()
    if not resolved_path or error_output:
        raise Exception(f"Failed to resolve remote path: {remote_expr_path}\nError: {error_output}")

    with sftp_client.open(resolved_path, mode='r') as remote_file:
        output = remote_file.read().decode('utf-8')
    sftp_client.close()
    ssh_client.close()
    
    data = defaultdict(dict)
    current_section = None
    if output:
        for line in output.splitlines():
            line = line.strip()
            if not line or line.startswith(('#', '//', ';')):
                continue  # skip comments and blank lines
            if line.startswith('[') and line.endswith(']'):
                current_section = line[1:-1]
                continue
            match = re.match(r'(\$\$?\w+)\s*=\s*(.*)', line)
            if match and current_section:
                key = match.group(1)
                value = match.group(2).strip()
                if value.lower() in ('true', 'false'):
                    value = value.lower() == 'true'
                else:
                    try:
                        if '.' in value:
                            value = float(value)
                        else:
                            value = int(value)
                    except ValueError:
                        value = value.strip('"').strip("'")  # treat as string
                data[current_section][key] = value
    else:
        return None
    return dict(data)
            

def is_existing_file(paramFile):

    if Path(paramFile).exists() and Path(paramFile).is_file():
        return paramFile
    else:
        return None
                  

def get_updated_session_parameters(session_param_json, mapping_default_param, wkfl_default_param, workflowParamFile, variables):
 
     if wkfl_default_param and session_param_json and not workflowParamFile and not mapping_default_param:
         variables.update(session_param_json)   
         return variables
     if wkfl_default_param and mapping_default_param and session_param_json and not workflowParamFile:  
         mapping_default_param.update(variables)
         mapping_default_param.update(session_param_json)
         return mapping_default_param
     if mapping_default_param and wkfl_default_param and not workflowParamFile and not session_param_json:
         variables.update(mapping_default_param)
         return variables
     if mapping_default_param and session_param_json and not wkfl_default_param and not workflowParamFile:
         mapping_default_param.update(session_param_json)
         return mapping_default_param
     if workflowParamFile and session_param_json and mapping_default_param:
         mapping_default_param.update(session_param_json)  
         mapping_default_param.update(variables or {})
         return mapping_default_param
     if workflowParamFile and session_param_json:
         session_param_json.update(workflowParamFile) 
         return session_param_json
     if wkfl_default_param and mapping_default_param and workflowParamFile and not session_param_json:
         mapping_default_param.update(variables)
         return mapping_default_param
     else:
         return mapping_default_param if mapping_default_param else variables if wkfl_default_param else {}
                  

def set_mapping_params(id, mapping_name, session_name, wkfl_name, session_param_json, workflowParamFile, variables):
    abs_path = '/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/data'
    mapping_file_path = is_existing_file(os.path.join(abs_path,"etc/prop/" + mapping_name + ".properties"))
    mapping_default_param = eval(open(os.path.join(abs_path,"etc/prop/" + mapping_name + ".properties")).read()) if mapping_file_path else None
    workflow_file_path = is_existing_file(os.path.join(abs_path,"etc/prop/" + wkfl_name + ".properties"))
    wkfl_default_param = workflow_file_path if workflow_file_path else None
    if session_param_json:
        if session_param_json.get(session_name):
            session_level_param_json = session_param_json.get(session_name)
        elif session_param_json.get(wkfl_name):
            session_level_param_json = session_param_json.get(wkfl_name).get(session_name)
        elif session_param_json.get('GLOBAL'):
            session_level_param_json = session_param_json.get('GLOBAL')
        else:
            session_level_param_json = {}
    else:
        session_level_param_json = {}
    var_dict = get_updated_session_parameters(session_level_param_json, mapping_default_param, wkfl_default_param, workflowParamFile, variables)
    keys = list(var_dict.keys())
    for k in range(len(var_dict)):
        key = keys[k]
        Variable.set(id + "." + key, var_dict[key])


class KeepAliveSSHHook(SSHHook):
    def get_conn(self):
        client = super().get_conn()
        client.get_transport().set_keepalive(int(Variable.get('KEEP_TIME_ALIVE')))
        return client

SSH_CONNECTION_ID = Variable.get('SSH_CONN_ID')
ssh_hook = KeepAliveSSHHook(ssh_conn_id=SSH_CONNECTION_ID)


def prepare_wklt_NORM_100_500_O_FIN_DAILY_DISC_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None) -> TaskGroup:
    with TaskGroup(group_id = groupId, default_args = dag.default_args if dag else parent_group.default_args) as task_group:
        Start = DummyOperator(
                task_id = 'Start',
                do_xcom_push = False,
        )

        @task(task_id = 's_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC_param_init_logic')
        @provide_session
        def fn_s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC_param_init_logic(session=None, **context):
            task_qualifiers = list(context['task'].downstream_task_ids)[0].split(".")
            del task_qualifiers[-1]
            group_id = ".".join(task_qualifiers)
            task_qualifiers = context['task'].task_id.split(".")
            del task_qualifiers[-1]
            current_group_id = ".".join(task_qualifiers)
            id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
            session_param_json = None
            variables = {}
            try:
                results = session.query(Variable).filter(Variable.key.like(f"{id}%")).all()
                variables = {var.key.replace(id + ".", "", 1): var.get_val() for var in results}
            finally:
                session.close()
        
            session_param_json = parse_prm_file('$PMRootDir/ETL_SCRIPTS/NORM/Params/aEDW_FIN_Disc_Daily.par')
            set_mapping_params(id + '.' + group_id, "m_100_CTL_ETL_FEED_PROCESS_START", "s_m_100_CTL_ETL_FEED_PROCESS_START", "dg_gk1v_edwdo_wln_mdr_billing_wkfl_norm_100_500_o_fin_daily_disc", session_param_json, "", variables)

        s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC__init = fn_s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC_param_init_logic()

        s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC = prepare_s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC_TG(parent_group = task_group, groupId = 's_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC', trigger = 'none_skipped')

        @task.branch(task_id = 'branch_logic_1', trigger_rule = 'none_skipped')
        def fn_branch_logic_1(**context):
            targets = []
            task_qualifiers = context['task'].task_id.split(".")
            del task_qualifiers[-1]
            current_group_id = ".".join(task_qualifiers)
            id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
            if "$s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC" == "succeeded":
                targets.append(current_group_id + '.Cmd_200_bbb_check_fact_load_NORM_FIN_DAILY_DISC')
            return targets

        branch_logic_1_task = fn_branch_logic_1()

        Cmd_200_bbb_check_fact_load_NORM_FIN_DAILY_DISC = SSHOperator(
                task_id = 'Cmd_200_bbb_check_fact_load_NORM_FIN_DAILY_DISC',
                ssh_hook = ssh_hook,
                command = "/usr/bin/ksh $ETL_HOME/ETL_SCRIPTS/SHARED_SCRIPTS/bbb_check_fact_load.ksh DW_FIN FIN_FACT_CUST_DISC MDR_DATA_CHURN_NORM ".format(task_group.group_id),
                conn_timeout = 60,
                get_pty = False,
                retries = 0,
                cmd_timeout = None,
                environment = {
          'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
          'instance_name': Variable.get('INSTANCE_NAME'),
          'job_id': "{{ dag.dag_id }}",
          'task_id': 'cmd_200_bbb_check_fact_load_norm_fin_daily_disc',
          'task_type': 'bq_to_bq_load'
        },
                doc_md = "",
                trigger_rule = 'all_success',
        )

        @task.branch(task_id = 'branch_logic_2', trigger_rule = 'none_skipped')
        def fn_branch_logic_2(**context):
            targets = []
            task_qualifiers = context['task'].task_id.split(".")
            del task_qualifiers[-1]
            current_group_id = ".".join(task_qualifiers)
            id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
            if (get_Status(context["dag_run"], current_group_id + '.' + "Cmd_200_bbb_check_fact_load_NORM_FIN_DAILY_DISC")) == "succeeded":
                targets.append(current_group_id + '.s_m_NORM_300_T_FIN_DAILY_DISC_param_init_logic')
            return targets

        branch_logic_2_task = fn_branch_logic_2()

        s_m_NORM_300_T_FIN_DAILY_DISC_Pre_session_command = SSHOperator(
                task_id = 's_m_NORM_300_T_FIN_DAILY_DISC_Pre_session_command',
                ssh_hook = ssh_hook,
                command = "$PMRootDir/ETL_SCRIPTS/SHARED_SCRIPTS/td_pre_cleanup.ksh DW_FIN_ADMIN FIN_TRN_DAILY_DISC INF_UTL_TBLS NORM_300_T_FIN_DAILY01_01 MLOAD $Param_LGConnectionLoad1 ".format(task_group.group_id),
                conn_timeout = 60,
                get_pty = False,
                retries = 0,
                cmd_timeout = None,
                environment = {
          'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
          'instance_name': Variable.get('INSTANCE_NAME'),
          'job_id': "{{ dag.dag_id }}",
          'task_id': 's_m_norm_300_t_fin_daily_disc_pre_session_command',
          'task_type': 'bq_to_bq_load'
        },
                doc_md = "",
                trigger_rule = 'all_success',
        )

        @task(task_id = 's_m_NORM_300_T_FIN_DAILY_DISC_param_init_logic')
        @provide_session
        def fn_s_m_NORM_300_T_FIN_DAILY_DISC_param_init_logic(session=None, **context):
            task_qualifiers = list(context['task'].downstream_task_ids)[0].split(".")
            del task_qualifiers[-1]
            group_id = ".".join(task_qualifiers)
            task_qualifiers = context['task'].task_id.split(".")
            del task_qualifiers[-1]
            current_group_id = ".".join(task_qualifiers)
            id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
            session_param_json = None
            variables = {}
            try:
                results = session.query(Variable).filter(Variable.key.like(f"{id}%")).all()
                variables = {var.key.replace(id + ".", "", 1): var.get_val() for var in results}
            finally:
                session.close()
        
            session_param_json = parse_prm_file('$PMRootDir/ETL_SCRIPTS/NORM/Params/aEDW_FIN_Disc_Daily.par')
            set_mapping_params(id + '.' + group_id, "m_NORM_300_T_FIN_DAILY_DISC", "s_m_NORM_300_T_FIN_DAILY_DISC", "dg_gk1v_edwdo_wln_mdr_billing_wkfl_norm_100_500_o_fin_daily_disc", session_param_json, "", variables)

        s_m_NORM_300_T_FIN_DAILY_DISC__init = fn_s_m_NORM_300_T_FIN_DAILY_DISC_param_init_logic()

        s_m_NORM_300_T_FIN_DAILY_DISC = prepare_s_m_NORM_300_T_FIN_DAILY_DISC_TG(parent_group = task_group, groupId = 's_m_NORM_300_T_FIN_DAILY_DISC', trigger = 'none_skipped')

        @task.branch(task_id = 'branch_logic_3', trigger_rule = 'none_skipped')
        def fn_branch_logic_3(**context):
            targets = []
            task_qualifiers = context['task'].task_id.split(".")
            del task_qualifiers[-1]
            current_group_id = ".".join(task_qualifiers)
            id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
            if Variable.get(id + '.' + current_group_id + '.s_m_NORM_300_T_FIN_DAILY_DISC.$JobStatus') == "succeeded":
                targets.append(current_group_id + '.s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT')
            return targets

        branch_logic_3_task = fn_branch_logic_3()

        s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT = prepare_s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT_TG(parent_group = task_group, groupId = 's_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT')

        @task.branch(task_id = 'branch_logic_4', trigger_rule = 'none_skipped')
        def fn_branch_logic_4(**context):
            targets = []
            task_qualifiers = context['task'].task_id.split(".")
            del task_qualifiers[-1]
            current_group_id = ".".join(task_qualifiers)
            id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
            if Variable.get(id + '.' + current_group_id + '.s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT.$JobStatus') == "succeeded":
                targets.append(current_group_id + '.cmd_400_sftp_NORM_FIN_DAILY_DISC')
            return targets

        branch_logic_4_task = fn_branch_logic_4()

        cmd_400_sftp_NORM_FIN_DAILY_DISC = SSHOperator(
                task_id = 'cmd_400_sftp_NORM_FIN_DAILY_DISC',
                ssh_hook = ssh_hook,
                command = "/usr/bin/ksh $ETL_HOME/ETL_SCRIPTS/NORM_LISCOUNT/Bin/sftp_norm_fin_daily_disc.ksh  ".format(task_group.group_id),
                conn_timeout = 60,
                get_pty = False,
                retries = 0,
                cmd_timeout = None,
                environment = {
          'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
          'instance_name': Variable.get('INSTANCE_NAME'),
          'job_id': "{{ dag.dag_id }}",
          'task_id': 'cmd_400_sftp_norm_fin_daily_disc',
          'task_type': 'bq_to_bq_load'
        },
                doc_md = "",
                trigger_rule = 'all_success',
        )

        @task.branch(task_id = 'branch_logic_5', trigger_rule = 'none_skipped')
        def fn_branch_logic_5(**context):
            targets = []
            task_qualifiers = context['task'].task_id.split(".")
            del task_qualifiers[-1]
            current_group_id = ".".join(task_qualifiers)
            id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
            if (get_Status(context["dag_run"], current_group_id + '.' + "cmd_400_sftp_NORM_FIN_DAILY_DISC")) == "succeeded":
                targets.append(current_group_id + '.s_m_500_CTL_ETL_FEED_PROCESS_END_param_init_logic')
            return targets

        branch_logic_5_task = fn_branch_logic_5()

        @task(task_id = 's_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC_param_init_logic')
        @provide_session
        def fn_s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC_param_init_logic(session=None, **context):
            task_qualifiers = list(context['task'].downstream_task_ids)[0].split(".")
            del task_qualifiers[-1]
            group_id = ".".join(task_qualifiers)
            task_qualifiers = context['task'].task_id.split(".")
            del task_qualifiers[-1]
            current_group_id = ".".join(task_qualifiers)
            id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
            session_param_json = None
            variables = {}
            try:
                results = session.query(Variable).filter(Variable.key.like(f"{id}%")).all()
                variables = {var.key.replace(id + ".", "", 1): var.get_val() for var in results}
            finally:
                session.close()
        
            session_param_json = parse_prm_file('$PMRootDir/ETL_SCRIPTS/NORM/Params/aEDW_FIN_Disc_Daily.par')
            set_mapping_params(id + '.' + group_id, "m_500_CTL_ETL_FEED_PROCESS_END", "s_m_500_CTL_ETL_FEED_PROCESS_END", "dg_gk1v_edwdo_wln_mdr_billing_wkfl_norm_100_500_o_fin_daily_disc", session_param_json, "", variables)

        s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC__init = fn_s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC_param_init_logic()

        s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC = prepare_s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC_TG(parent_group = task_group, groupId = 's_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC', trigger = 'none_skipped')

        s_m_NORM_300_T_FIN_DAILY_DISC_Post_session_success_command = SSHOperator(
                task_id = 's_m_NORM_300_T_FIN_DAILY_DISC_Post_session_success_command',
                ssh_hook = ssh_hook,
                command = "$PMRootDir/ETL_SCRIPTS/SHARED_SCRIPTS/td_post_check.ksh DW_FIN_ADMIN FIN_TRN_DAILY_DISC INF_UTL_TBLS NORM_300_T_FIN_DAILY01_01 MLOAD $Param_LGConnectionLoad1 ".format(task_group.group_id),
                conn_timeout = 60,
                get_pty = False,
                retries = 0,
                cmd_timeout = None,
                environment = {
          'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
          'instance_name': Variable.get('INSTANCE_NAME'),
          'job_id': "{{ dag.dag_id }}",
          'task_id': 's_m_norm_300_t_fin_daily_disc_post_session_success_command',
          'task_type': 'bq_to_bq_load'
        },
                doc_md = "",
                trigger_rule = 'all_success',
        )

        status = DMStatusOperator(
                task_id = 'status',
                group_id = task_group.group_id,
                jobName = 'wklt_NORM_100_500_O_FIN_DAILY_DISC',
                userStatus = userStatus,
                trigger_rule = 'all_done',
        )


        Start >> s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC__init >> s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC >> branch_logic_1_task >> Cmd_200_bbb_check_fact_load_NORM_FIN_DAILY_DISC >> s_m_NORM_300_T_FIN_DAILY_DISC_Pre_session_command >> branch_logic_2_task >> s_m_NORM_300_T_FIN_DAILY_DISC__init >> s_m_NORM_300_T_FIN_DAILY_DISC
        s_m_NORM_300_T_FIN_DAILY_DISC >> branch_logic_3_task >> s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT >> branch_logic_4_task >> cmd_400_sftp_NORM_FIN_DAILY_DISC >> branch_logic_5_task >> s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC__init >> s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC >> status
        s_m_NORM_300_T_FIN_DAILY_DISC >> s_m_NORM_300_T_FIN_DAILY_DISC_Post_session_success_command >> status
    return task_group
