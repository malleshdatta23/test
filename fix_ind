@task
def gcs_file_processor(
    gcs_bucket_name,
    gcs_object_path,
    data_path,
    archive_path,
    init_file_name,
    output_file_prefix,
    output_file_suffix,
    date_time_str,
    mail_recipient=None,
    header_file_name=None,
    trailer_file_name=None,
    **kwargs
):
    # Normalize object path
    if not gcs_object_path:
        gcs_object_path = ""

    logging.info(f"gcs_bucket_name: {gcs_bucket_name}")
    logging.info(f"gcs_object_path: {gcs_object_path}")

    storage_client = get_storage_client()

    # Remove gs:// prefix
    data_gcs = gcs_bucket_name.replace("gs://", "")
    bucket = storage_client.bucket(data_gcs)

    # Archive bucket derivation
    archive_gcs = data_gcs.replace("outbound", "archive")
    archive_bucket = storage_client.bucket(archive_gcs)

    full_data_path = f"{gcs_object_path}{data_path}"
    full_archive_path = f"{gcs_object_path}{archive_path}"

    # ----------------------------------------------------
    # Dynamic outbound file discovery (lines_*.out)
    # ----------------------------------------------------
    prefix = os.path.join(full_data_path, "lines_")
    matching_blobs = list(bucket.list_blobs(prefix=prefix))

    if not matching_blobs:
        raise FileNotFoundError(
            f"No outbound files found with prefix '{prefix}' in bucket {data_gcs}"
        )

    # Pick latest generated file
    INIT_FILE_BLOB = sorted(
        matching_blobs,
        key=lambda b: b.updated or b.time_created,
        reverse=True
    )[0]

    logging.info(f"Selected outbound file: {INIT_FILE_BLOB.name}")

    # ----------------------------------------------------
    # Header / trailer handling
    # ----------------------------------------------------
    source_blobs_to_compose = []
    compose_needed = False

    if header_file_name:
        HEADER_BLOB = bucket.blob(os.path.join(full_data_path, header_file_name))
        source_blobs_to_compose.append(HEADER_BLOB)
        compose_needed = True

    source_blobs_to_compose.append(INIT_FILE_BLOB)

    if trailer_file_name:
        TRAILER_BLOB = bucket.blob(os.path.join(full_data_path, trailer_file_name))
        source_blobs_to_compose.append(TRAILER_BLOB)
        compose_needed = True

    # ----------------------------------------------------
    # Target file naming
    # ----------------------------------------------------
    run_date_part = str(date_time_str)[:8]
    SRC_FILE_NAME = (
        f"{output_file_prefix}_{run_date_part}_{date_time_str}{output_file_suffix}"
    )

    GCS_PROCESSING_BLOB_NAME = os.path.join(full_data_path, SRC_FILE_NAME)
    GCS_PROCESSING_BLOB = bucket.blob(GCS_PROCESSING_BLOB_NAME)

    ARCHIVE_BLOB_NAME = os.path.join(full_archive_path, SRC_FILE_NAME)
    ARCHIVE_BLOB = archive_bucket.blob(ARCHIVE_BLOB_NAME)

    # ----------------------------------------------------
    # Validation & processing
    # ----------------------------------------------------
    try:
        logging.info("Starting file availability and size validation...")

        try:
            INIT_FILE_BLOB.reload(client=storage_client)
        except NotFound as e:
            raise FileNotFoundError(
                f"Data file not available. GCS Error: {e}"
            )

        if INIT_FILE_BLOB.size is None:
            raise FileNotFoundError("Data file size could not be determined")
        elif INIT_FILE_BLOB.size == 0:
            raise ValueError(
                f"Data file ({INIT_FILE_BLOB.name}) has zero bytes. Process aborted."
            )

        logging.info(f"Data file size check passed: {INIT_FILE_BLOB.size} bytes")

        # Compose or copy
        if compose_needed:
            GCS_PROCESSING_BLOB.compose(source_blobs_to_compose)
            logging.info(
                f"Composition successful: {GCS_PROCESSING_BLOB.name}"
            )
        else:
            bucket.copy_blob(
                INIT_FILE_BLOB, bucket, new_name=GCS_PROCESSING_BLOB.name
            )
            logging.info(
                f"File copied to processing location: {GCS_PROCESSING_BLOB.name}"
            )

        # Archive
        logging.info(f"Archiving file to: {ARCHIVE_BLOB_NAME}")
        bucket.copy_blob(
            GCS_PROCESSING_BLOB, archive_bucket, new_name=ARCHIVE_BLOB_NAME
        )

        # Cleanup
        GCS_PROCESSING_BLOB.delete()
        logging.info(f"Deleted processing file: {GCS_PROCESSING_BLOB.name}")

        logging.info("GCS file processing completed successfully.")

    except (FileNotFoundError, ValueError) as e:
        logging.error(f"GCS processing FAILED: {e}", exc_info=True)
        raise

    except Exception as e:
        logging.error(f"Unexpected error during GCS processing: {e}", exc_info=True)
        raise
