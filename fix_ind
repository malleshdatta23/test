# import sys
# sys.path.append('/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/')
# from airflow.models import Variable
# import airflow
# from airflow import DAG
# from airflow.utils.task_group import TaskGroup
# from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
# import os
# from airflow.providers.ssh.operators.ssh import SSHOperator
# from airflow.providers.ssh.hooks.ssh import SSHHook
# from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
# from google.cloud import bigquery
# from operators.dm_status import DMStatusOperator
# abs_path ='/home/airflow/gcs/data/sql/'

# from airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator

# GCS_OUTBOUND = Variable.get('GCS_OUTBOUND')
# bq_project_id = Variable.get('BQ_ProjectID')

# def get_row_stats(context):
#     task_id = context["task"].task_id
#     # default conn_id = google_cloud_default
#     hook = GoogleBaseHook(gcp_conn_id=Variable.get('GCP_CONNECTION_ID'))
#     credentials = hook.get_credentials()
#     client = bigquery.Client(credentials=credentials)
#     job_id = context["ti"].xcom_pull(task_ids=task_id, key='return_value')
#     job = client.get_job(job_id, location="us", project=Variable.get('BQ_ProjectID'))
#     child_jobs = list(client.list_jobs(parent_job=job.job_id, project=job.project))

#     if child_jobs:
#         for child in child_jobs:
#             set_job_stats(child, context, client)
#     else:
#         set_job_stats(job, context, client)

# def set_job_stats(job, context, client):
#     # fetch tgt row count
#     if job.destination and "__tmp__" not in job.destination.table_id:
#         id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
#         tgt_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', default_var=0)
#         current_tgt_row_count = job.dml_stats.inserted_row_count if job.dml_stats else 0
#         tgt_row_count = int(tgt_row_count) + current_tgt_row_count
#         Variable.set(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', tgt_row_count)
    
#     # fetch src row count
#     if job.destination or job.referenced_tables:
#         id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
#         src_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', default_var=0)
#         src_tables = [
#             table
#             for table in job.referenced_tables
#             if job.destination and table.table_id != job.destination.table_id and "__tmp__" not in table.table_id
#         ]
#         if src_tables == []:
#             src_tables.append(job.destination)
#         for table in src_tables:
#             table_name = table.table_id
#             project = table.project
#             dataset = table.dataset_id
            
#             try:
#                 table_ref = client.get_table(f"{project}.{dataset}.{table_name}")
#                 current_src_row_count = table_ref.num_rows
#                 src_row_count = int(src_row_count) + current_src_row_count
#                 Variable.set(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', src_row_count)
#             except Exception as e:
#                 print(f"Failed to fetch row count for {table_name}: {e}")

# # class KeepAliveSSHHook(SSHHook):
# #     def get_conn(self):
# #         client = super().get_conn()
# #         client.get_transport().set_keepalive(int(Variable.get('KEEP_TIME_ALIVE')))
# #         return client

# # SSH_CONNECTION_ID = Variable.get('SSH_CONN_ID')
# # ssh_hook = KeepAliveSSHHook(ssh_conn_id=SSH_CONNECTION_ID)

# def prepare_s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None, bq_project_id = None, bq_project_pr_id = None) -> TaskGroup:
#     with TaskGroup(group_id = groupId, default_args = dag.default_args if dag else parent_group.default_args) as task_group:
#         m_NORM_320_O_FIN_DAILY_DISC_OUTPUT = BigQueryInsertJobOperator(
#                 task_id = 'm_NORM_320_O_FIN_DAILY_DISC_OUTPUT',
#                 project_id = bq_project_id,
#                 configuration = {
#                         'query': {
#                                 'query': open(os.path.join(abs_path,"dataMapping/MDR_Billing/wkfl_NORM_100_500_O_FIN_DAILY_DISC/wklt_NORM_100_500_O_FIN_DAILY_DISC/s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT/m_NORM_320_O_FIN_DAILY_DISC_OUTPUT","m_NORM_320_O_FIN_DAILY_DISC_OUTPUT.sql"), "r").read().format(task_group.group_id),
#                                 'useLegacySql': False,
#                                 'params': {
#                                     'BQ_ProjectID': bq_project_id,
#                                     'BQ_ProjectID_PR': bq_project_pr_id
#                                 }
#                         } ,
#                         'labels': {
#                                 'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
#                                 'instance_name': "{{ var.value.INSTANCE_NAME }}",
#                                 'job_id' : 'wkfl_norm_100_500_o_fin_daily_disc',
#                                 'task_id': 'm_norm_320_o_fin_daily_disc_output',
#                                 'task_type': 'bq_to_bq_load'
#                         }
#                 },
#                 gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
#                 job_id = 'wkfl_NORM_100_500_O_FIN_DAILY_DISC_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT',
#                 on_success_callback = get_row_stats,
#                 trigger_rule = trigger,
#         )

#         # export_File_1060236703 = SSHOperator(
#         #         task_id = 'File_1060236703',
#         #         ssh_hook = ssh_hook,
#         #         command = f"""bq_query_execute $PMRootDir/MDR_FEEDS/NORM_LISCOUNT/Data/fin_daily_disc.txt $PMRootDir/MDR_FEEDS/NORM_LISCOUNT/Data/fin_daily_disc.out txt true '|' 'select * from {bq_project_id.replace("{", "{{").replace("}", "}}")}.aedw360_tbls.file_1060236703' file true """.format(task_group.group_id),
#         #         conn_timeout = 60,
#         #         get_pty = False,
#         #         retries = 0,
#         #         cmd_timeout = None,
#         #         environment = {
#         #   'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
#         #   'instance_name': "{{ var.value.INSTANCE_NAME }}",
#         #   'job_id': "{{ dag.dag_id }}",
#         #   'task_id': 'file_1060236703',
#         #   'task_type': 'sshoperator'
#         # },
#         #         doc_md = "",
#         #         trigger_rule = 'all_success',
#         # )

#         # export_File_389123673 = SSHOperator(
#         #         task_id = 'File_389123673',
#         #         ssh_hook = ssh_hook,
#         #         command = f"""bq_query_execute $PMRootDir/MDR_FEEDS/NORM_LISCOUNT/Data/fin_daily_disc_header.txt $PMRootDir/MDR_FEEDS/NORM_LISCOUNT/Data/fin_daily_disc_header.out txt true ',' 'select * from {bq_project_id.replace("{", "{{").replace("}", "}}")}.aedw360_tbls.file_389123673' file true """.format(task_group.group_id),
#         #         conn_timeout = 60,
#         #         get_pty = False,
#         #         retries = 0,
#         #         cmd_timeout = None,
#         #         environment = {
#         #   'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
#         #   'instance_name': "{{ var.value.INSTANCE_NAME }}",
#         #   'job_id': "{{ dag.dag_id }}",
#         #   'task_id': 'file_389123673',
#         #   'task_type': 'sshoperator'
#         # },
#         #         doc_md = "",
#         #         trigger_rule = 'all_success',
#         # )
#         table_id = f"{bq_project_id}.aedw360_tbls.file_1320985894"
#         gcs_object_path = "non_dpf/mapping_output_as_file/sysmnt/apps/opt/informatica/10.5.1/server/infa_shared/etluser/"
#         gcs_destination_uri = f"{GCS_OUTBOUND}/{gcs_object_path}"

#         export_File_1060236703 = BigQueryToGCSOperator(
#                 task_id = 'File_1060236703',
#                 source_project_dataset_table = table_id,
#                 destination_cloud_storage_uris = [gcs_destination_uri],
#                 export_format = 'CSV', 
#                 field_delimiter = '|',
#                 print_header = True, 
#                 gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
#                 trigger_rule = 'all_success',
#         )

#         # EXPORT 2: HEADER FILE
#         table_id = f"{bq_project_id}.aedw360_tbls.file_389123673"
#         gcs_object_path = "non_dpf/mapping_output_as_file/sysmnt/apps/opt/informatica/10.5.1/server/infa_shared/etluser/MDR_FEEDS/NORM_LISCOUNT/Data/fin_daily_disc_header.txt"
#         gcs_destination_uri = f"{GCS_OUTBOUND}/{gcs_object_path}"

#         export_File_389123673 = BigQueryToGCSOperator(
#                 task_id = 'File_389123673',
#                 source_project_dataset_table = table_id,
#                 destination_cloud_storage_uris = [gcs_destination_uri],
#                 export_format = 'CSV', 
#                 field_delimiter = ',',
#                 print_header = True, 
#                 gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
#                 trigger_rule = 'all_success',
#         )

#         status = DMStatusOperator(
#                 task_id = 'status',
#                 group_id = task_group.group_id,
#                 jobName = 'm_NORM_320_O_FIN_DAILY_DISC_OUTPUT',
#                 userStatus = userStatus,
#                 trigger_rule = 'all_done',
#         )
#         m_NORM_320_O_FIN_DAILY_DISC_OUTPUT >> export_File_1060236703 >> export_File_389123673 >> status
#     return task_group


import sys
sys.path.append('/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/')
from airflow.models import Variable
from airflow import DAG
from airflow.utils.task_group import TaskGroup
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
from airflow.providers.google.cloud.transfers.bigquery_to_gcs import BigQueryToGCSOperator
from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery
from operators.dm_status import DMStatusOperator
import os
import logging

abs_path ='/home/airflow/gcs/data/sql/'

def get_row_stats(context):
    task_id = context["task"].task_id
    hook = GoogleBaseHook(gcp_conn_id=Variable.get('GCP_CONNECTION_ID'))
    credentials = hook.get_credentials()
    client = bigquery.Client(credentials=credentials)
    job_id = context["ti"].xcom_pull(task_ids=task_id, key='return_value')
    job = client.get_job(job_id, location="us", project=Variable.get('BQ_ProjectID'))
    child_jobs = list(client.list_jobs(parent_job=job.job_id, project=job.project))
    if child_jobs:
        for child in child_jobs:
            set_job_stats(child, context, client)
    else:
        set_job_stats(job, context, client)

def set_job_stats(job, context, client):
    if job.destination and "__tmp__" not in job.destination.table_id:
        id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
        tgt_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', default_var=0)
        current_tgt_row_count = job.dml_stats.inserted_row_count if job.dml_stats else 0
        tgt_row_count = int(tgt_row_count) + current_tgt_row_count
        Variable.set(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', tgt_row_count)
    if job.destination or job.referenced_tables:
        id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
        src_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', default_var=0)
        src_tables = [table for table in job.referenced_tables if job.destination and table.table_id != job.destination.table_id and "__tmp__" not in table.table_id]
        if src_tables == []:
            src_tables.append(job.destination)
        for table in src_tables:
            table_name = table.table_id
            try:
                table_ref = client.get_table(f"{table.project}.{table.dataset_id}.{table_name}")
                src_row_count = int(src_row_count) + table_ref.num_rows
                Variable.set(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', src_row_count)
            except Exception as e:
                logging.error(f"Failed row count for {table_name}: {e}")

def prepare_s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None, bq_project_id = None, bq_project_pr_id = None) -> TaskGroup:
    with TaskGroup(group_id = groupId, default_args = dag.default_args if dag else parent_group.default_args) as task_group:
        
        m_NORM_320_O_FIN_DAILY_DISC_OUTPUT = BigQueryInsertJobOperator(
            task_id = 'm_NORM_320_O_FIN_DAILY_DISC_OUTPUT',
            project_id = bq_project_id,
            on_execute_callback = forensic_audit_log,
            configuration = {
                'query': {
                    'query': open(os.path.join(abs_path,"dataMapping/MDR_Billing/wkfl_NORM_100_500_O_FIN_DAILY_DISC/wklt_NORM_100_500_O_FIN_DAILY_DISC/s_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT/m_NORM_320_O_FIN_DAILY_DISC_OUTPUT","m_NORM_320_O_FIN_DAILY_DISC_OUTPUT.sql"), "r").read().format(task_group.group_id),
                    'useLegacySql': False,
                    'params': {
                        'BQ_ProjectID': bq_project_id,
                        'BQ_ProjectID_PR': bq_project_pr_id
                    }
                },
                'labels': {
                    'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
                    'instance_name': "{{ var.value.INSTANCE_NAME }}",
                    'job_id' : 'wkfl_norm_100_500_o_fin_daily_disc',
                    'task_id': 'm_norm_320_o_fin_daily_disc_output',
                    'task_type': 'bq_to_bq_load'
                }
            },
            gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
            job_id = 'wkfl_NORM_100_500_O_FIN_DAILY_DISC_m_NORM_320_O_FIN_DAILY_DISC_OUTPUT',
            on_success_callback = get_row_stats,
            trigger_rule = trigger,
        )

        GCS_OUTBOUND = Variable.get('GCS_OUTBOUND')

        # EXPORT 1: DATA FILE (FIXED TABLE AND PATH)
        table_id_1 = f"{bq_project_id}.aedw360_tbls.file_1060236703"
        gcs_obj_1 = "non_dpf/mapping_output_as_file/sysmnt/apps/opt/informatica/10.5.1/server/infa_shared/etluser/MDR_FEEDS/NORM_LISCOUNT/Data/fin_daily_disc.txt"
        gcs_uri_1 = f"gs://{GCS_OUTBOUND}/{gcs_obj_1}"

        export_File_1060236703 = BigQueryToGCSOperator(
            task_id = 'File_1060236703',
            source_project_dataset_table = table_id_1,
            destination_cloud_storage_uris = [gcs_uri_1],
            export_format = 'CSV', 
            field_delimiter = '|',
            print_header = True, 
            gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
            trigger_rule = 'all_success',
        )

        # EXPORT 2: HEADER FILE
        table_id_2 = f"{bq_project_id}.aedw360_tbls.file_389123673"
        gcs_obj_2 = "non_dpf/mapping_output_as_file/sysmnt/apps/opt/informatica/10.5.1/server/infa_shared/etluser/MDR_FEEDS/NORM_LISCOUNT/Data/fin_daily_disc_header.txt"
        gcs_uri_2 = f"gs://{GCS_OUTBOUND}/{gcs_obj_2}"

        export_File_389123673 = BigQueryToGCSOperator(
            task_id = 'File_389123673',
            source_project_dataset_table = table_id_2,
            destination_cloud_storage_uris = [gcs_uri_2],
            export_format = 'CSV', 
            field_delimiter = ',',
            print_header = True, 
            gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
            trigger_rule = 'all_success',
        )

        status = DMStatusOperator(
            task_id = 'status',
            group_id = task_group.group_id,
            jobName = 'm_NORM_320_O_FIN_DAILY_DISC_OUTPUT',
            userStatus = userStatus,
            trigger_rule = 'all_done',
        )

        m_NORM_320_O_FIN_DAILY_DISC_OUTPUT >> export_File_1060236703 >> export_File_389123673 >> status

    return task_group
