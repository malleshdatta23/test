import sys
sys.path.append('/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/')
from airflow.models import Variable
import airflow
from airflow import DAG
from airflow.utils.task_group import TaskGroup
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
import os
from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery
from operators.dm_status import DMStatusOperator
abs_path ='/home/airflow/gcs/data/sql/'

def get_row_stats(context):
    task_id = context["task"].task_id
    # default conn_id = google_cloud_default
    hook = GoogleBaseHook(gcp_conn_id=Variable.get('GCP_CONNECTION_ID'))
    credentials = hook.get_credentials()
    client = bigquery.Client(credentials=credentials)
    job_id = context["ti"].xcom_pull(task_ids=task_id, key='return_value')
    job = client.get_job(job_id, location="us", project=Variable.get('BQ_ProjectID'))
    child_jobs = list(client.list_jobs(parent_job=job.job_id, project=job.project))

    if child_jobs:
        for child in child_jobs:
            set_job_stats(child, context, client)
    else:
        set_job_stats(job, context, client)

def set_job_stats(job, context, client):
    # fetch tgt row count
    if job.destination and "__tmp__" not in job.destination.table_id:
        id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
        tgt_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', default_var=0)
        current_tgt_row_count = job.dml_stats.inserted_row_count if job.dml_stats else 0
        tgt_row_count = int(tgt_row_count) + current_tgt_row_count
        Variable.set(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', tgt_row_count)
    
    # fetch src row count
    if job.destination or job.referenced_tables:
        id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
        src_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', default_var=0)
        src_tables = [
            table
            for table in job.referenced_tables
            if job.destination and table.table_id != job.destination.table_id and "__tmp__" not in table.table_id
        ]
        if src_tables == []:
            src_tables.append(job.destination)
        for table in src_tables:
            table_name = table.table_id
            project = table.project
            dataset = table.dataset_id
            
            try:
                table_ref = client.get_table(f"{project}.{dataset}.{table_name}")
                current_src_row_count = table_ref.num_rows
                src_row_count = int(src_row_count) + current_src_row_count
                Variable.set(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', src_row_count)
            except Exception as e:
                print(f"Failed to fetch row count for {table_name}: {e}")


# def prepare_s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None, bq_project_id = None, bq_project_pr_id = None) -> TaskGroup:
#     with TaskGroup(group_id = groupId, default_args = dag.default_args if dag else parent_group.default_args) as task_group:
#         m_500_CTL_ETL_FEED_PROCESS_END = BigQueryInsertJobOperator(
#                 task_id = 'm_500_CTL_ETL_FEED_PROCESS_END',
#                 project_id = bq_project_id,
#                 configuration = {
#                         'query': {
#                                 'query': open(os.path.join(abs_path,"dataMapping/MDR_Billing/wkfl_NORM_100_500_O_FIN_DAILY_DISC/wklt_NORM_100_500_O_FIN_DAILY_DISC/s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC/m_500_CTL_ETL_FEED_PROCESS_END","m_500_CTL_ETL_FEED_PROCESS_END.sql"), "r").read().format(task_group.group_id),
#                                 'useLegacySql': False,
#                                 'params': {
#                                     'BQ_ProjectID': bq_project_id,
#                                     'BQ_ProjectID_PR': bq_project_pr_id
#                                 }
#                         } ,
#                         'labels': {
#                                 'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
#                                 'instance_name': "{{ var.value.INSTANCE_NAME }}",
#                                 'job_id' : 'wkfl_norm_100_500_o_fin_daily_disc',
#                                 'task_id': 'm_500_ctl_etl_feed_process_end',
#                                 'task_type': 'bq_to_bq_load'
#                         }
#                 },
#                 gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
#                 job_id = 'wkfl_NORM_100_500_O_FIN_DAILY_DISC_m_500_CTL_ETL_FEED_PROCESS_END',
#                 on_success_callback = get_row_stats,
#                 trigger_rule = trigger,
#         )

#         status = DMStatusOperator(
#                 task_id = 'status',
#                 group_id = task_group.group_id,
#                 jobName = 'm_500_CTL_ETL_FEED_PROCESS_END',
#                 userStatus = userStatus,
#                 trigger_rule = 'all_done',
#         )
#         m_500_CTL_ETL_FEED_PROCESS_END >> status
#     return task_group
def prepare_s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None, bq_project_id = None, bq_project_pr_id = None) -> TaskGroup:
    with TaskGroup(group_id = groupId, default_args = dag.default_args if dag else parent_group.default_args) as task_group:
        
        m_500_CTL_ETL_FEED_PROCESS_END = BigQueryInsertJobOperator(
            task_id = 'm_500_CTL_ETL_FEED_PROCESS_END',
            project_id = bq_project_id,
            on_execute_callback = forensic_audit_log, # ATTACHED: Forensic Audit
            configuration = {
                'query': {
                    'query': open(os.path.join(abs_path,"dataMapping/MDR_Billing/wkfl_NORM_100_500_O_FIN_DAILY_DISC/wklt_NORM_100_500_O_FIN_DAILY_DISC/s_m_500_CTL_ETL_FEED_PROCESS_END_NORM_FIN_DAILY_DISC/m_500_CTL_ETL_FEED_PROCESS_END","m_500_CTL_ETL_FEED_PROCESS_END.sql"), "r").read().format(task_group.group_id),
                    'useLegacySql': False,
                    'params': {
                        'BQ_ProjectID': bq_project_id,
                        'BQ_ProjectID_PR': bq_project_pr_id,
                        # FIXED: Removed 'street' and corrected Jinja braces
                        'FEED_NAME': "{{ var.value.get(dag.dag_id ~ '_' ~ dag_run.id ~ '." + groupId + ".$$FEED_NAME') }}",
                        'TGT_FACT_TABLE': "{{ var.value.get(dag.dag_id ~ '_' ~ dag_run.id ~ '." + groupId + ".$$TGT_FACT_TABLE') }}",
                        'TGT_FACT_SCHEMA': "{{ var.value.get(dag.dag_id ~ '_' ~ dag_run.id ~ '." + groupId + ".$$TGT_FACT_SCHEMA') }}"
                    }
                },
                'labels': {
                    'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
                    'instance_name': "{{ var.value.INSTANCE_NAME }}",
                    'job_id' : 'wkfl_norm_100_500_o_fin_daily_disc',
                    'task_id': 'm_500_ctl_etl_feed_process_end',
                    'task_type': 'bq_to_bq_load'
                }
            },
            gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
            job_id = 'wkfl_NORM_100_500_O_FIN_DAILY_DISC_m_500_CTL_ETL_FEED_PROCESS_END',
            on_success_callback = get_row_stats,
            trigger_rule = trigger,
        )

        status = DMStatusOperator(
            task_id = 'status',
            group_id = task_group.group_id,
            jobName = 'm_500_CTL_ETL_FEED_PROCESS_END',
            userStatus = userStatus,
            trigger_rule = 'all_done',
        )

        m_500_CTL_ETL_FEED_PROCESS_END >> status

    return task_group
