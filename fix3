# import sys
# sys.path.append('/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/')
# from airflow.models import Variable
# import airflow
# from airflow import DAG
# from airflow.utils.task_group import TaskGroup
# from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
# import os
# from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
# from google.cloud import bigquery
# from operators.dm_status import DMStatusOperator
# abs_path ='/home/airflow/gcs/data/sql/'

# def get_row_stats(context):
#     task_id = context["task"].task_id
#     # default conn_id = google_cloud_default
#     hook = GoogleBaseHook(gcp_conn_id=Variable.get('GCP_CONNECTION_ID'))
#     credentials = hook.get_credentials()
#     client = bigquery.Client(credentials=credentials)
#     job_id = context["ti"].xcom_pull(task_ids=task_id, key='return_value')
#     job = client.get_job(job_id, location="us", project=Variable.get('BQ_ProjectID'))
#     child_jobs = list(client.list_jobs(parent_job=job.job_id, project=job.project))

#     if child_jobs:
#         for child in child_jobs:
#             set_job_stats(child, context, client)
#     else:
#         set_job_stats(job, context, client)

# def set_job_stats(job, context, client):
#     # fetch tgt row count
#     if job.destination and "__tmp__" not in job.destination.table_id:
#         id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
#         tgt_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', default_var=0)
#         current_tgt_row_count = job.dml_stats.inserted_row_count if job.dml_stats else 0
#         tgt_row_count = int(tgt_row_count) + current_tgt_row_count
#         Variable.set(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', tgt_row_count)
    
#     # fetch src row count
#     if job.destination or job.referenced_tables:
#         id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
#         src_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', default_var=0)
#         src_tables = [
#             table
#             for table in job.referenced_tables
#             if job.destination and table.table_id != job.destination.table_id and "__tmp__" not in table.table_id
#         ]
#         if src_tables == []:
#             src_tables.append(job.destination)
#         for table in src_tables:
#             table_name = table.table_id
#             project = table.project
#             dataset = table.dataset_id
            
#             try:
#                 table_ref = client.get_table(f"{project}.{dataset}.{table_name}")
#                 current_src_row_count = table_ref.num_rows
#                 src_row_count = int(src_row_count) + current_src_row_count
#                 Variable.set(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', src_row_count)
#             except Exception as e:
#                 print(f"Failed to fetch row count for {table_name}: {e}")


# # def prepare_s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None, bq_project_id = None, bq_project_pr_id = None) -> TaskGroup:
# #                 task_id = 'm_100_CTL_ETL_FEED_PROCESS_START',
# #                 project_id = bq_project_id,
# #                 configuration = {
# #                         'query': {
# #                                 'query': open(os.path.join(abs_path,"dataMapping/MDR_Billing/wkfl_NORM_100_500_O_FIN_DAILY_DISC/wklt_NORM_100_500_O_FIN_DAILY_DISC/s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC/m_100_CTL_ETL_FEED_PROCESS_START","m_100_CTL_ETL_FEED_PROCESS_START.sql"), "r").read().format(task_group.group_id),
# #                                 'useLegacySql': False,
# #                                 'params': {
# #                                     'BQ_ProjectID': bq_project_id,
# #                                     'BQ_ProjectID_PR': bq_project_pr_id
# #                                 }
# #                         } ,
# #                         'labels': {
# #                                 'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
# #                                 'instance_name': "{{ var.value.INSTANCE_NAME }}",
# #                                 'job_id' : 'wkfl_norm_100_500_o_fin_daily_disc',
# #                                 'task_id': 'm_100_ctl_etl_feed_process_start',
# #                                 'task_type': 'bq_to_bq_load'
# #                         }
# #                 },
# #                 gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
# #                 job_id = 'wkfl_NORM_100_500_O_FIN_DAILY_DISC_m_100_CTL_ETL_FEED_PROCESS_START',
# #                 on_success_callback = get_row_stats,
# #                 trigger_rule = trigger,

# #         status = DMStatusOperator(
# #                 task_id = 'status',
# #                 group_id = task_group.group_id,
# #                 jobName = 'm_100_CTL_ETL_FEED_PROCESS_START',
# #                 userStatus = userStatus,
# #                 trigger_rule = 'all_done',
# #         )
# #         m_100_CTL_ETL_FEED_PROCESS_START >> status
# #     return task_group
# import sys
# sys.path.append('/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/')
# from airflow.models import Variable
# import airflow
# from airflow import DAG
# from airflow.utils.task_group import TaskGroup
# from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
# import os
# from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
# from google.cloud import bigquery
# from operators.dm_status import DMStatusOperator
# abs_path ='/home/airflow/gcs/data/sql/'

# def get_row_stats(context):
#     task_id = context["task"].task_id
#     # default conn_id = google_cloud_default
#     hook = GoogleBaseHook(gcp_conn_id=Variable.get('GCP_CONNECTION_ID'))
#     credentials = hook.get_credentials()
#     client = bigquery.Client(credentials=credentials)
#     job_id = context["ti"].xcom_pull(task_ids=task_id, key='return_value')
#     job = client.get_job(job_id, location="us", project=Variable.get('BQ_ProjectID'))
#     child_jobs = list(client.list_jobs(parent_job=job.job_id, project=job.project))

#     if child_jobs:
#         for child in child_jobs:
#             set_job_stats(child, context, client)
#     else:
#         set_job_stats(job, context, client)

# def set_job_stats(job, context, client):
#     # fetch tgt row count
#     if job.destination and "__tmp__" not in job.destination.table_id:
#         id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
#         tgt_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', default_var=0)
#         current_tgt_row_count = job.dml_stats.inserted_row_count if job.dml_stats else 0
#         tgt_row_count = int(tgt_row_count) + current_tgt_row_count
#         Variable.set(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', tgt_row_count)
    
#     # fetch src row count
#     if job.destination or job.referenced_tables:
#         id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
#         src_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', default_var=0)
#         src_tables = [
#             table
#             for table in job.referenced_tables
#             if job.destination and table.table_id != job.destination.table_id and "__tmp__" not in table.table_id
#         ]
#         if src_tables == []:
#             src_tables.append(job.destination)
#         for table in src_tables:
#             table_name = table.table_id
#             project = table.project
#             dataset = table.dataset_id
            
#             try:
#                 table_ref = client.get_table(f"{project}.{dataset}.{table_name}")
#                 current_src_row_count = table_ref.num_rows
#                 src_row_count = int(src_row_count) + current_src_row_count
#                 Variable.set(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', src_row_count)
#             except Exception as e:
#                 print(f"Failed to fetch row count for {table_name}: {e}")


# def prepare_s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None, bq_project_id = None, bq_project_pr_id = None) -> TaskGroup:
#                 task_id = 'm_100_CTL_ETL_FEED_PROCESS_START',
#                 project_id = bq_project_id,
#                 configuration = {
#                         'query': {
#                                 'query': open(os.path.join(abs_path,"dataMapping/MDR_Billing/wkfl_NORM_100_500_O_FIN_DAILY_DISC/wklt_NORM_100_500_O_FIN_DAILY_DISC/s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC/m_100_CTL_ETL_FEED_PROCESS_START","m_100_CTL_ETL_FEED_PROCESS_START.sql"), "r").read().format(task_group.group_id),
#                                 'useLegacySql': False,
#                                 'params': {
#                                     'BQ_ProjectID': bq_project_id,
#                                     'BQ_ProjectID_PR': bq_project_pr_id
#                                 }
#                         } ,
#                         'labels': {
#                                 'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
#                                 'instance_name': "{{ var.value.INSTANCE_NAME }}",
#                                 'job_id' : 'wkfl_norm_100_500_o_fin_daily_disc',
#                                 'task_id': 'm_100_ctl_etl_feed_process_start',
#                                 'task_type': 'bq_to_bq_load'
#                         }
#                 },
#                 gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
#                 job_id = 'wkfl_NORM_100_500_O_FIN_DAILY_DISC_m_100_CTL_ETL_FEED_PROCESS_START',
#                 on_success_callback = get_row_stats,
#                 trigger_rule = trigger,

#         status = DMStatusOperator(
#                 task_id = 'status',
#                 group_id = task_group.group_id,
#                 jobName = 'm_100_CTL_ETL_FEED_PROCESS_START',
#                 userStatus = userStatus,
#                 trigger_rule = 'all_done',
#         )
#         m_100_CTL_ETL_FEED_PROCESS_START >> status
#     return task_group

import sys
sys.path.append('/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/')
from airflow.models import Variable
import airflow
from airflow import DAG
from airflow.utils.task_group import TaskGroup
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
import os
from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery
from operators.dm_status import DMStatusOperator
abs_path ='/home/airflow/gcs/data/sql/'

def get_row_stats(context):
    task_id = context["task"].task_id
    # default conn_id = google_cloud_default
    hook = GoogleBaseHook(gcp_conn_id=Variable.get('GCP_CONNECTION_ID'))
    credentials = hook.get_credentials()
    client = bigquery.Client(credentials=credentials)
    job_id = context["ti"].xcom_pull(task_ids=task_id, key='return_value')
    job = client.get_job(job_id, location="us", project=Variable.get('BQ_ProjectID'))
    child_jobs = list(client.list_jobs(parent_job=job.job_id, project=job.project))

    if child_jobs:
        for child in child_jobs:
            set_job_stats(child, context, client)
    else:
        set_job_stats(job, context, client)

def set_job_stats(job, context, client):
    # fetch tgt row count
    if job.destination and "__tmp__" not in job.destination.table_id:
        id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
        tgt_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', default_var=0)
        current_tgt_row_count = job.dml_stats.inserted_row_count if job.dml_stats else 0
        tgt_row_count = int(tgt_row_count) + current_tgt_row_count
        Variable.set(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', tgt_row_count)
    
    # fetch src row count
    if job.destination or job.referenced_tables:
        id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
        src_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', default_var=0)
        src_tables = [
            table
            for table in job.referenced_tables
            if job.destination and table.table_id != job.destination.table_id and "__tmp__" not in table.table_id
        ]
        if src_tables == []:
            src_tables.append(job.destination)
        for table in src_tables:
            table_name = table.table_id
            project = table.project
            dataset = table.dataset_id
            
            try:
                table_ref = client.get_table(f"{project}.{dataset}.{table_name}")
                current_src_row_count = table_ref.num_rows
                src_row_count = int(src_row_count) + current_src_row_count
                Variable.set(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', src_row_count)
            except Exception as e:
                print(f"Failed to fetch row count for {table_name}: {e}")


# def prepare_s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None, bq_project_id = None, bq_project_pr_id = None) -> TaskGroup:
#                 task_id = 'm_100_CTL_ETL_FEED_PROCESS_START',
#                 project_id = bq_project_id,
#                 configuration = {
#                         'query': {
#                                 'query': open(os.path.join(abs_path,"dataMapping/MDR_Billing/wkfl_NORM_100_500_O_FIN_DAILY_DISC/wklt_NORM_100_500_O_FIN_DAILY_DISC/s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC/m_100_CTL_ETL_FEED_PROCESS_START","m_100_CTL_ETL_FEED_PROCESS_START.sql"), "r").read().format(task_group.group_id),
#                                 'useLegacySql': False,
#                                 'params': {
#                                     'BQ_ProjectID': bq_project_id,
#                                     'BQ_ProjectID_PR': bq_project_pr_id
#                                 }
#                         } ,
#                         'labels': {
#                                 'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
#                                 'instance_name': "{{ var.value.INSTANCE_NAME }}",
#                                 'job_id' : 'wkfl_norm_100_500_o_fin_daily_disc',
#                                 'task_id': 'm_100_ctl_etl_feed_process_start',
#                                 'task_type': 'bq_to_bq_load'
#                         }
#                 },
#                 gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
#                 job_id = 'wkfl_NORM_100_500_O_FIN_DAILY_DISC_m_100_CTL_ETL_FEED_PROCESS_START',
#                 on_success_callback = get_row_stats,
#                 trigger_rule = trigger,

#         status = DMStatusOperator(
#                 task_id = 'status',
#                 group_id = task_group.group_id,
#                 jobName = 'm_100_CTL_ETL_FEED_PROCESS_START',
#                 userStatus = userStatus,
#                 trigger_rule = 'all_done',
#         )
#         m_100_CTL_ETL_FEED_PROCESS_START >> status
#     return task_group
import sys
sys.path.append('/home/airflow/gcs/dags/vz-it-gk1v-edwdo-0/')
from airflow.models import Variable
import airflow
from airflow import DAG
from airflow.utils.task_group import TaskGroup
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator
import os
from airflow.providers.google.common.hooks.base_google import GoogleBaseHook
from google.cloud import bigquery
from operators.dm_status import DMStatusOperator
abs_path ='/home/airflow/gcs/data/sql/'

def get_row_stats(context):
    task_id = context["task"].task_id
    # default conn_id = google_cloud_default
    hook = GoogleBaseHook(gcp_conn_id=Variable.get('GCP_CONNECTION_ID'))
    credentials = hook.get_credentials()
    client = bigquery.Client(credentials=credentials)
    job_id = context["ti"].xcom_pull(task_ids=task_id, key='return_value')
    job = client.get_job(job_id, location="us", project=Variable.get('BQ_ProjectID'))
    child_jobs = list(client.list_jobs(parent_job=job.job_id, project=job.project))

    if child_jobs:
        for child in child_jobs:
            set_job_stats(child, context, client)
    else:
        set_job_stats(job, context, client)

def set_job_stats(job, context, client):
    # fetch tgt row count
    if job.destination and "__tmp__" not in job.destination.table_id:
        id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
        tgt_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', default_var=0)
        current_tgt_row_count = job.dml_stats.inserted_row_count if job.dml_stats else 0
        tgt_row_count = int(tgt_row_count) + current_tgt_row_count
        Variable.set(id + '.' + context["task"].task_group.group_id + '.TgtSuccessRows', tgt_row_count)
    
    # fetch src row count
    if job.destination or job.referenced_tables:
        id = context['dag_run'].dag_id + '_' + str(context['dag_run'].id)
        src_row_count = Variable.get(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', default_var=0)
        src_tables = [
            table
            for table in job.referenced_tables
            if job.destination and table.table_id != job.destination.table_id and "__tmp__" not in table.table_id
        ]
        if src_tables == []:
            src_tables.append(job.destination)
        for table in src_tables:
            table_name = table.table_id
            project = table.project
            dataset = table.dataset_id
            
            try:
                table_ref = client.get_table(f"{project}.{dataset}.{table_name}")
                current_src_row_count = table_ref.num_rows
                src_row_count = int(src_row_count) + current_src_row_count
                Variable.set(id + '.' + context["task"].task_group.group_id + '.SrcSuccessRows', src_row_count)
            except Exception as e:
                print(f"Failed to fetch row count for {table_name}: {e}")


def prepare_s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC_TG(groupId, trigger = 'all_success', parent_group = None, dag: DAG = None, userStatus = None, bq_project_id = None, bq_project_pr_id = None) -> TaskGroup:
    # 1. FIXED: Restored the TaskGroup context
    with TaskGroup(group_id = groupId, default_args = dag.default_args if dag else parent_group.default_args) as task_group:
        
        # 2. FIXED: Restored the Operator Class name and opening parenthesis
        m_100_CTL_ETL_FEED_PROCESS_START = BigQueryInsertJobOperator(
            task_id = 'm_100_CTL_ETL_FEED_PROCESS_START',
            project_id = bq_project_id,
            on_execute_callback = forensic_audit_log, # Attached for your log audit
            configuration = {
                'query': {
                    'query': open(os.path.join(abs_path,"dataMapping/MDR_Billing/wkfl_NORM_100_500_O_FIN_DAILY_DISC/wklt_NORM_100_500_O_FIN_DAILY_DISC/s_m_100_CTL_ETL_FEED_PROCESS_START_NORM_FIN_DAILY_DISC/m_100_CTL_ETL_FEED_PROCESS_START","m_100_CTL_ETL_FEED_PROCESS_START.sql"), "r").read().format(task_group.group_id),
                    'useLegacySql': False,
                    'params': {
                        'BQ_ProjectID': bq_project_id,
                        'BQ_ProjectID_PR': bq_project_pr_id
                    }
                },
                'labels': {
                    'run_id': "{{ dag_run.run_id | lower | replace(':', '') | replace('+', '') | replace('.', '') | replace('-', '') | replace('T', '') }}",
                    'instance_name': "{{ var.value.INSTANCE_NAME }}",
                    'job_id' : 'wkfl_norm_100_500_o_fin_daily_disc',
                    'task_id': 'm_100_ctl_etl_feed_process_start',
                    'task_type': 'bq_to_bq_load'
                }
            },
            gcp_conn_id = Variable.get('GCP_CONNECTION_ID'),
            job_id = 'wkfl_NORM_100_500_O_FIN_DAILY_DISC_m_100_CTL_ETL_FEED_PROCESS_START',
            on_success_callback = get_row_stats,
            trigger_rule = trigger,
        ) # 3. FIXED: Properly closed the operator

        status = DMStatusOperator(
            task_id = 'status',
            group_id = task_group.group_id,
            jobName = 'm_100_CTL_ETL_FEED_PROCESS_START',
            userStatus = userStatus,
            trigger_rule = 'all_done',
        )

        # 4. FIXED: Set dependencies inside the with block
        m_100_CTL_ETL_FEED_PROCESS_START >> status

    return task_group
